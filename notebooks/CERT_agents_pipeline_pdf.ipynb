{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW8UKTx1YNjr"
      },
      "source": [
        "##**Claude-Only CERT Agents Pipeline Pipeline Coordination Demo**\n",
        "---\n",
        "\n",
        "This framework provides systematic measurement of coordination patterns in current LLM systems. While these systems manipulate discrete tokens based on statistical correlations rather than learning continuous representations of the physical world, this infrastructure will be essential when we develop architectures based on learned world models.\n",
        "\n",
        "**What this measures:**\n",
        "Coordination behaviors between sophisticated pattern-matching systems.\n",
        "\n",
        "**What this enables:**\n",
        "Deployment scaffolding for current technology + experimental apparatus for studying genuine coordination when it emerges from proper architectures.\n",
        "\n",
        "###**Basic description**\n",
        "\n",
        ">Predetermined sequential flow:\n",
        "\n",
        ">> Agent 1 ‚Üí Agent 2 ‚Üí ... ‚Üí Agent N for (N<10)\n",
        "\n",
        ">Fixed processing chain with no autonomous decision-making\n",
        "\n",
        ">Each \"agent\" is a role-specialized LLM instance processing predetermined inputs\n",
        "\n",
        ">No dynamic coordination or emergent communication patterns\n",
        "\n",
        "###**Setup Instructions**\n",
        "####1. API Configuration\n",
        "python# Set your Anthropic API key\n",
        "api_key = \"sk-ant-api03-your-key-here\"\n",
        "####2. Agent Configuration (2-10 agents)\n",
        "Each agent requires four parameters:\n",
        "\n",
        ">Agent ID: Unique identifier (agent_1, summarizer, etc.)\n",
        "\n",
        ">Model: Choose from available Claude models\n",
        "\n",
        ">Role: Agent's specialized function (Document Analyst, Critical Reviewer)\n",
        "\n",
        ">Task: Specific instructions for this agent's analysis approach\n",
        "\n",
        "Available Models:\n",
        "```\n",
        "claude-opus-4-20250514 - #Most powerful, complex reasoning\n",
        "claude-sonnet-4-20250514 - #Balanced performance (recommended)\n",
        "claude-3-5-haiku-20241022 - #Fastest response times\n",
        "claude-3-haiku-20240307 - #Legacy model for comparison\n",
        "```\n",
        "####3. Coordination Configuration\n",
        "Global Task: Overall objective for the Agents Pipeline system\n",
        "\n",
        "Coordination Pattern:\n",
        ">Sequential: Agents process in order, building on previous outputs\n",
        "\n",
        ">Parallel: All agents analyze simultaneously, independent perspectives\n",
        "\n",
        "####4. Document Upload\n",
        "Upload PDF documents for analysis. The system extracts text content and uses it as context for agent coordination.\n",
        "\n",
        "###**Execution Process**\n",
        "\n",
        "####Phase 1: Individual Analysis\n",
        "\n",
        ">**Behavioral Consistency Score ($C$)**\n",
        "\n",
        "\n",
        ">How reliably an agent produces similar responses to identical tasks.\n",
        "\n",
        ">for $C$ in the range 0.9-1.0: Highly reliable\n",
        "\n",
        ">for $C$ in the range 0.7-0.9: Moderately consistent\n",
        "\n",
        ">for $C$<0.7: Unreliable for deployment\n",
        "\n",
        "\n",
        "####Phase 2: Agents Pipeline Coordination\n",
        "Agents coordinate according to selected pattern while system tracks:\n",
        "\n",
        ">Conversation flow: Complete interaction sequence\n",
        "\n",
        ">Response quality: Success/failure rates\n",
        "\n",
        ">Timing patterns: Response latencies and bottlenecks\n",
        "\n",
        "####Phase 3: Coordination Effect Measurement\n",
        ">**Coordination Effect ($\\gamma$)**\n",
        "\n",
        ">Performance change when agents work together vs. alone.\n",
        "\n",
        "<center>$\\gamma = \\frac{\\textrm{Coordinated Performance}}{\\textrm{Individual Performance}}$</center>\n",
        "\n",
        "> $\\gamma$ > 1.0: Agents help each other\n",
        "\n",
        "> $\\gamma$ = 1.0: No coordination benefit\n",
        "\n",
        "> $\\gamma$ < 1.0: Agents interfere with each other\n",
        "\n",
        "###**Interactive Visualization**\n",
        "Conversation Timeline: Real-time tracking of agent interactions with step-by-step conversation flow\n",
        "performance four-panel analysis showing:\n",
        "\n",
        "> Agent consistency scores over time\n",
        "\n",
        "> Response time patterns by agent\n",
        "\n",
        "> Coordination effects across experiments\n",
        "\n",
        "> Success rates and error analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzbede_Gbt_d"
      },
      "source": [
        "## Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZgzcI_STnuT"
      },
      "outputs": [],
      "source": [
        "##Install required packages and clone the CERT repository##\n",
        "#!pip install anthropic transformers torch dotenv pycryptodome PyPDF2\n",
        "#!pip install -q watermark\n",
        "## Clone CERT repository##\n",
        "#!git clone https://github.com/Javihaus/cert-coordination-observability.git\n",
        "#!cd cert-coordination-observability && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7-dyLTydsUh"
      },
      "outputs": [],
      "source": [
        "%load_ext watermark\n",
        "%watermark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GnRnjaJeCXZ"
      },
      "source": [
        "**Test environment**\n",
        "\n",
        "Python implementation: CPython <br>\n",
        "Python version       : 3.11.13<br>\n",
        "IPython version      : 7.34.0<br>\n",
        "\n",
        "Compiler    : GCC 11.4.0<br>\n",
        "OS          : Linux<br>\n",
        "Release     : 6.1.123+<br>\n",
        "Machine     : x86_64<br>\n",
        "Processor   : x86_64<br>\n",
        "CPU cores   : 2<br>\n",
        "Architecture: 64bit<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARyVt-NoXQk_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import anthropic\n",
        "import PyPDF2\n",
        "import io\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPZsk67bXZqd"
      },
      "outputs": [],
      "source": [
        "ANTHROPIC_API_KEY=\"xxxxxxxx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YZQloAzddQt"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqnhQtZuqFJb"
      },
      "outputs": [],
      "source": [
        "# CERT Core Measurement Components\n",
        "@dataclass\n",
        "class AgentInteraction:\n",
        "    timestamp: datetime\n",
        "    agent_id: str\n",
        "    model: str\n",
        "    role: str\n",
        "    task: str\n",
        "    prompt: str\n",
        "    response: str\n",
        "    response_time: float\n",
        "    success: bool\n",
        "    error: Optional[str] = None\n",
        "    metadata: Dict[str, Any] = None\n",
        "\n",
        "@dataclass\n",
        "class CoordinationStep:\n",
        "    step_number: int\n",
        "    agent_id: str\n",
        "    input_context: str\n",
        "    output: str\n",
        "    reasoning: str\n",
        "    timestamp: datetime\n",
        "\n",
        "class CERTMeasurement:\n",
        "    \"\"\"Core measurement logic for behavioral consistency and coordination effects\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_behavioral_consistency(responses: List[str]) -> float:\n",
        "        \"\"\"Calculate consistency score (Œ≤) from multiple responses - IMPROVED VERSION\"\"\"\n",
        "        if len(responses) < 2:\n",
        "            return 1.0\n",
        "\n",
        "        # Improved consistency measurement that handles semantic similarity\n",
        "        # Filter out common stopwords that don't contribute to content meaning\n",
        "        stopwords = {\n",
        "            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
        "            'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
        "            'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',\n",
        "            'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'\n",
        "        }\n",
        "\n",
        "        # Extract meaningful content words from each response\n",
        "        content_sets = []\n",
        "        for response in responses:\n",
        "            # Clean and tokenize\n",
        "            cleaned = response.lower()\n",
        "            # Remove punctuation and numbers that might vary between responses\n",
        "            import re\n",
        "            cleaned = re.sub(r'[^\\w\\s]', ' ', cleaned)\n",
        "            cleaned = re.sub(r'\\d+', '', cleaned)  # Remove numbers like \"1.\", \"2.\", etc.\n",
        "\n",
        "            # Extract meaningful words (length > 2, not stopwords)\n",
        "            words = cleaned.split()\n",
        "            content_words = [w for w in words if len(w) > 2 and w not in stopwords]\n",
        "            content_sets.append(set(content_words))\n",
        "\n",
        "        # Calculate pairwise content similarity\n",
        "        similarities = []\n",
        "        for i in range(len(content_sets)):\n",
        "            for j in range(i + 1, len(content_sets)):\n",
        "                intersection = len(content_sets[i].intersection(content_sets[j]))\n",
        "                union = len(content_sets[i].union(content_sets[j]))\n",
        "\n",
        "                if union == 0:\n",
        "                    similarity = 1.0  # Both responses had no meaningful content\n",
        "                else:\n",
        "                    # Use Jaccard similarity but boost it for structured tasks\n",
        "                    jaccard = intersection / union\n",
        "\n",
        "                    # For extraction tasks, high content overlap should score higher\n",
        "                    # Apply a boost function that rewards high content overlap\n",
        "                    if jaccard > 0.5:\n",
        "                        similarity = 0.5 + (jaccard - 0.5) * 1.5  # Boost high similarity\n",
        "                    else:\n",
        "                        similarity = jaccard\n",
        "\n",
        "                    similarity = min(1.0, similarity)  # Cap at 1.0\n",
        "\n",
        "                similarities.append(similarity)\n",
        "\n",
        "        return np.mean(similarities) if similarities else 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_coordination_effect(individual_performances: List[float],\n",
        "                                    coordinated_performance: float) -> float:\n",
        "        \"\"\"Calculate coordination effect (Œ≥) - IMPROVED VERSION\"\"\"\n",
        "\n",
        "        # Handle edge cases\n",
        "        if not individual_performances or len(individual_performances) == 0:\n",
        "            return 1.0\n",
        "\n",
        "        expected_performance = np.mean(individual_performances)\n",
        "\n",
        "        # Avoid division by zero\n",
        "        if expected_performance == 0:\n",
        "            if coordinated_performance > 0:\n",
        "                return 2.0  # Some improvement over zero baseline\n",
        "            else:\n",
        "                return 1.0  # No change from zero baseline\n",
        "\n",
        "        gamma = coordinated_performance / expected_performance\n",
        "\n",
        "        # Apply reasonable bounds to prevent extreme values due to measurement noise\n",
        "        # Real coordination effects typically range from 0.5 to 2.0\n",
        "        gamma = max(0.1, min(gamma, 3.0))\n",
        "\n",
        "        return gamma\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_response_quality(response: str, task_type: str = \"extraction\") -> float:\n",
        "        \"\"\"Calculate quality score for individual responses\"\"\"\n",
        "\n",
        "        if not response or len(response.strip()) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        if task_type == \"extraction\":\n",
        "            # For extraction tasks like \"list three main points\"\n",
        "            # Quality = completeness + structure + content richness\n",
        "\n",
        "            # Check for list structure (numbered, bulleted, or clear separation)\n",
        "            structure_score = 0.0\n",
        "            response_lower = response.lower()\n",
        "\n",
        "            # Look for list indicators\n",
        "            list_indicators = ['1.', '2.', '3.', '‚Ä¢', '-', 'first', 'second', 'third', 'one:', 'two:', 'three:']\n",
        "            structure_count = sum(1 for indicator in list_indicators if indicator in response_lower)\n",
        "\n",
        "            if structure_count >= 2:\n",
        "                structure_score = 0.3  # Well-structured response\n",
        "            elif structure_count >= 1:\n",
        "                structure_score = 0.1  # Some structure\n",
        "\n",
        "            # Content richness (meaningful words)\n",
        "            words = response.split()\n",
        "            meaningful_words = [w for w in words if len(w) > 3]\n",
        "            content_score = min(0.4, len(meaningful_words) / 20.0)  # Cap at 0.4\n",
        "\n",
        "            # Completeness (length indicator)\n",
        "            length_score = min(0.3, len(response) / 200.0)  # Cap at 0.3\n",
        "\n",
        "            total_quality = structure_score + content_score + length_score\n",
        "            return min(1.0, total_quality)\n",
        "\n",
        "        elif task_type == \"analysis\":\n",
        "            # For analytical tasks - different quality metrics\n",
        "            words = response.split()\n",
        "\n",
        "            # Analytical depth indicators\n",
        "            analytical_terms = ['because', 'therefore', 'however', 'moreover', 'analysis', 'conclusion', 'evidence']\n",
        "            analytical_score = sum(1 for term in analytical_terms if term in response.lower()) / 10.0\n",
        "\n",
        "            # Content richness\n",
        "            content_score = min(0.6, len(words) / 50.0)\n",
        "\n",
        "            total_quality = min(1.0, analytical_score + content_score)\n",
        "            return total_quality\n",
        "\n",
        "        else:\n",
        "            # Default quality metric\n",
        "            word_count = len(response.split())\n",
        "            if word_count < 10:\n",
        "                return 0.2\n",
        "            elif word_count < 50:\n",
        "                return 0.5 + (word_count - 10) / 80.0  # Linear increase\n",
        "            else:\n",
        "                return 0.8 + min(0.2, (word_count - 50) / 200.0)  # Diminishing returns\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_pipeline_performance(responses: List[str], task_type: str = \"extraction\") -> float:\n",
        "        \"\"\"Calculate overall performance of a pipeline of responses\"\"\"\n",
        "\n",
        "        if not responses:\n",
        "            return 0.0\n",
        "\n",
        "        # For pipeline, the final response is most important\n",
        "        # But we also consider the progression quality\n",
        "\n",
        "        individual_qualities = [\n",
        "            CERTMeasurement.calculate_response_quality(response, task_type)\n",
        "            for response in responses\n",
        "        ]\n",
        "\n",
        "        if len(individual_qualities) == 1:\n",
        "            return individual_qualities[0]\n",
        "\n",
        "        # Weight the final response more heavily, but consider progression\n",
        "        final_weight = 0.6\n",
        "        progression_weight = 0.4\n",
        "\n",
        "        final_quality = individual_qualities[-1]\n",
        "        progression_quality = np.mean(individual_qualities[:-1]) if len(individual_qualities) > 1 else 0\n",
        "\n",
        "        pipeline_performance = (final_weight * final_quality) + (progression_weight * progression_quality)\n",
        "\n",
        "        return pipeline_performance\n",
        "\n",
        "class ClaudeAgent:\n",
        "    \"\"\"Individual Claude agent with specific role and model\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str, model: str, role: str, task_description: str, api_key: str):\n",
        "        self.agent_id = agent_id\n",
        "        self.model = model\n",
        "        self.role = role\n",
        "        self.task_description = task_description\n",
        "        self.client = anthropic.Anthropic(api_key=api_key)\n",
        "        self.interaction_history = []\n",
        "        self.performance_metrics = {\n",
        "            \"consistency_scores\": [],\n",
        "            \"response_times\": [],\n",
        "            \"error_count\": 0,\n",
        "            \"success_count\": 0\n",
        "        }\n",
        "\n",
        "    async def generate_response(self, prompt: str, context: str = \"\",\n",
        "                              max_tokens: int = 500) -> AgentInteraction:\n",
        "        \"\"\"Generate response and track performance metrics\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            full_prompt = f\"\"\"Role: {self.role}\n",
        "Task: {self.task_description}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Request: {prompt}\n",
        "\n",
        "Respond according to your role and task. Be concise but thorough.\"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=self.model,\n",
        "                max_tokens=max_tokens,\n",
        "                messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
        "            )\n",
        "\n",
        "            response_time = time.time() - start_time\n",
        "            response_text = response.content[0].text\n",
        "\n",
        "            interaction = AgentInteraction(\n",
        "                timestamp=datetime.now(),\n",
        "                agent_id=self.agent_id,\n",
        "                model=self.model,\n",
        "                role=self.role,\n",
        "                task=self.task_description,\n",
        "                prompt=prompt,\n",
        "                response=response_text,\n",
        "                response_time=response_time,\n",
        "                success=True,\n",
        "                metadata={\"context\": context, \"max_tokens\": max_tokens}\n",
        "            )\n",
        "\n",
        "            self.interaction_history.append(interaction)\n",
        "            self.performance_metrics[\"success_count\"] += 1\n",
        "            self.performance_metrics[\"response_times\"].append(response_time)\n",
        "\n",
        "            return interaction\n",
        "\n",
        "        except Exception as e:\n",
        "            response_time = time.time() - start_time\n",
        "\n",
        "            interaction = AgentInteraction(\n",
        "                timestamp=datetime.now(),\n",
        "                agent_id=self.agent_id,\n",
        "                model=self.model,\n",
        "                role=self.role,\n",
        "                task=self.task_description,\n",
        "                prompt=prompt,\n",
        "                response=\"\",\n",
        "                response_time=response_time,\n",
        "                success=False,\n",
        "                error=str(e)\n",
        "            )\n",
        "\n",
        "            self.interaction_history.append(interaction)\n",
        "            self.performance_metrics[\"error_count\"] += 1\n",
        "\n",
        "            return interaction\n",
        "\n",
        "    async def measure_consistency(self, prompt: str, trials: int = 3) -> float:\n",
        "        \"\"\"Measure behavioral consistency across multiple trials\"\"\"\n",
        "        responses = []\n",
        "\n",
        "        for _ in range(trials):\n",
        "            interaction = await self.generate_response(prompt)\n",
        "            if interaction.success:\n",
        "                responses.append(interaction.response)\n",
        "            await asyncio.sleep(0.5)  # Rate limiting\n",
        "\n",
        "        if len(responses) >= 2:\n",
        "            consistency = CERTMeasurement.calculate_behavioral_consistency(responses)\n",
        "            self.performance_metrics[\"consistency_scores\"].append(consistency)\n",
        "            return consistency\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "class CoordinationOrchestrator:\n",
        "    \"\"\"Manages Agents Pipeline coordination and conversation tracking\"\"\"\n",
        "\n",
        "    def __init__(self, agents: List[ClaudeAgent]):\n",
        "        self.agents = agents\n",
        "        self.coordination_history = []\n",
        "        self.conversation_log = []\n",
        "        self.coordination_effects = []\n",
        "\n",
        "    async def run_sequential_coordination(self, initial_prompt: str,\n",
        "                                        document_content: str = \"\") -> List[CoordinationStep]:\n",
        "        \"\"\"Run sequential coordination between agents\"\"\"\n",
        "        coordination_steps = []\n",
        "        current_context = f\"Document: {document_content}\\n\\nInitial Task: {initial_prompt}\"\n",
        "\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            step_prompt = f\"Step {i+1}: {current_context}\"\n",
        "\n",
        "            interaction = await agent.generate_response(step_prompt, current_context)\n",
        "\n",
        "            step = CoordinationStep(\n",
        "                step_number=i + 1,\n",
        "                agent_id=agent.agent_id,\n",
        "                input_context=current_context,\n",
        "                output=interaction.response if interaction.success else f\"ERROR: {interaction.error}\",\n",
        "                reasoning=f\"Agent {agent.agent_id} ({agent.role}) processing step {i+1}\",\n",
        "                timestamp=interaction.timestamp\n",
        "            )\n",
        "\n",
        "            coordination_steps.append(step)\n",
        "            self.conversation_log.append({\n",
        "                \"step\": i + 1,\n",
        "                \"agent\": agent.agent_id,\n",
        "                \"role\": agent.role,\n",
        "                \"model\": agent.model,\n",
        "                \"task\": agent.task_description,\n",
        "                \"input\": current_context[:200] + \"...\" if len(current_context) > 200 else current_context,\n",
        "                \"output\": step.output,\n",
        "                \"timestamp\": step.timestamp,\n",
        "                \"success\": interaction.success,\n",
        "                \"response_time\": interaction.response_time\n",
        "            })\n",
        "\n",
        "            # Update context for next agent\n",
        "            if interaction.success:\n",
        "                current_context = f\"Previous analysis: {interaction.response}\\n\\nContinue the analysis:\"\n",
        "\n",
        "            await asyncio.sleep(1)  # Rate limiting between agents\n",
        "\n",
        "        self.coordination_history.extend(coordination_steps)\n",
        "        return coordination_steps\n",
        "\n",
        "    async def run_parallel_coordination(self, task_prompt: str,\n",
        "                                      document_content: str = \"\") -> List[CoordinationStep]:\n",
        "        \"\"\"Run parallel coordination where all agents work simultaneously\"\"\"\n",
        "        coordination_steps = []\n",
        "        context = f\"Document: {document_content}\\n\\nTask: {task_prompt}\"\n",
        "\n",
        "        # All agents process the same prompt simultaneously\n",
        "        tasks = []\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            tasks.append(agent.generate_response(task_prompt, context))\n",
        "\n",
        "        interactions = await asyncio.gather(*tasks)\n",
        "\n",
        "        for i, (agent, interaction) in enumerate(zip(self.agents, interactions)):\n",
        "            step = CoordinationStep(\n",
        "                step_number=i + 1,\n",
        "                agent_id=agent.agent_id,\n",
        "                input_context=context,\n",
        "                output=interaction.response if interaction.success else f\"ERROR: {interaction.error}\",\n",
        "                reasoning=f\"Agent {agent.agent_id} ({agent.role}) parallel processing\",\n",
        "                timestamp=interaction.timestamp\n",
        "            )\n",
        "\n",
        "            coordination_steps.append(step)\n",
        "            self.conversation_log.append({\n",
        "                \"step\": i + 1,\n",
        "                \"agent\": agent.agent_id,\n",
        "                \"role\": agent.role,\n",
        "                \"model\": agent.model,\n",
        "                \"task\": agent.task_description,\n",
        "                \"input\": context[:200] + \"...\" if len(context) > 200 else context,\n",
        "                \"output\": step.output,\n",
        "                \"timestamp\": step.timestamp,\n",
        "                \"success\": interaction.success,\n",
        "                \"response_time\": interaction.response_time\n",
        "            })\n",
        "\n",
        "        self.coordination_history.extend(coordination_steps)\n",
        "        return coordination_steps\n",
        "\n",
        "    def measure_coordination_effect(self, coordination_steps: List[CoordinationStep]) -> float:\n",
        "        \"\"\"Measure overall coordination effect\"\"\"\n",
        "        # Get individual baseline performances\n",
        "        individual_performances = []\n",
        "        for agent in self.agents:\n",
        "            if agent.performance_metrics[\"consistency_scores\"]:\n",
        "                individual_performances.append(np.mean(agent.performance_metrics[\"consistency_scores\"]))\n",
        "            else:\n",
        "                individual_performances.append(0.5)  # Default baseline\n",
        "\n",
        "        # Simulate coordinated performance based on successful steps\n",
        "        successful_steps = [step for step in coordination_steps if \"ERROR\" not in step.output]\n",
        "        coordinated_performance = len(successful_steps) / len(coordination_steps) if coordination_steps else 0\n",
        "\n",
        "        gamma = CERTMeasurement.calculate_coordination_effect(individual_performances, coordinated_performance)\n",
        "\n",
        "        self.coordination_effects.append({\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"individual_performances\": individual_performances,\n",
        "            \"coordinated_performance\": coordinated_performance,\n",
        "            \"coordination_effect\": gamma,\n",
        "            \"successful_steps\": len(successful_steps),\n",
        "            \"total_steps\": len(coordination_steps)\n",
        "        })\n",
        "\n",
        "        return gamma\n",
        "\n",
        "class CERTVisualizer:\n",
        "    \"\"\"Creates interactive visualizations of coordination behavior\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_agent_color(agent_index: int, total_agents: int) -> str:\n",
        "        \"\"\"Generate distinct colors for any number of agents\"\"\"\n",
        "        # Use a color palette that works well for 2-10 agents\n",
        "        color_palette = [\n",
        "            \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\",\n",
        "            \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"\n",
        "        ]\n",
        "        return color_palette[agent_index % len(color_palette)]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_agent_icon(agent_index: int) -> str:\n",
        "        \"\"\"Generate distinct icons for any number of agents\"\"\"\n",
        "        icons = [\"üîç\", \"‚ö°\", \"üéØ\", \"üß†\", \"üìä\", \"üî¨\", \"üí°\", \"üé®\", \"üìà\", \"üîß\"]\n",
        "        return icons[agent_index % len(icons)]\n",
        "\n",
        "    @staticmethod\n",
        "    def display_agent_conversation(conversation_log: List[Dict]):\n",
        "        \"\"\"Display clear agent conversation flow with formatted text\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ü§ù AGENT CONVERSATION FLOW\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Create agent-to-index mapping for consistent coloring\n",
        "        unique_agents = []\n",
        "        for entry in conversation_log:\n",
        "            if entry[\"agent\"] not in unique_agents:\n",
        "                unique_agents.append(entry[\"agent\"])\n",
        "\n",
        "        for entry in conversation_log:\n",
        "            step_num = entry[\"step\"]\n",
        "            agent_id = entry[\"agent\"]\n",
        "            role = entry[\"role\"]\n",
        "            output = entry[\"output\"]\n",
        "            success = entry[\"success\"]\n",
        "\n",
        "            # Get consistent color and icon for this agent\n",
        "            agent_index = unique_agents.index(agent_id)\n",
        "            icon = CERTVisualizer.get_agent_icon(agent_index)\n",
        "\n",
        "            # ANSI color codes for terminal output\n",
        "            color_codes = [\n",
        "                \"\\033[94m\",  # Blue\n",
        "                \"\\033[91m\",  # Red\n",
        "                \"\\033[92m\",  # Green\n",
        "                \"\\033[93m\",  # Yellow\n",
        "                \"\\033[95m\",  # Magenta\n",
        "                \"\\033[96m\",  # Cyan\n",
        "                \"\\033[97m\",  # White\n",
        "                \"\\033[90m\",  # Gray\n",
        "                \"\\033[94m\",  # Blue (repeat for >8 agents)\n",
        "                \"\\033[91m\"   # Red\n",
        "            ]\n",
        "\n",
        "            color = color_codes[agent_index % len(color_codes)]\n",
        "            reset_color = \"\\033[0m\"\n",
        "\n",
        "            print(f\"\\n{color}{'='*60}{reset_color}\")\n",
        "            print(f\"{color}{icon} STEP {step_num}: {agent_id.upper()} ({role}){reset_color}\")\n",
        "            print(f\"{color}{'='*60}{reset_color}\")\n",
        "\n",
        "            if success:\n",
        "                # Clean and format the output\n",
        "                cleaned_output = output.strip()\n",
        "                if len(cleaned_output) > 1000:\n",
        "                    # Show first part and indicate truncation\n",
        "                    print(f\"{cleaned_output[:1000]}...\")\n",
        "                    print(f\"\\n{color}[Output truncated - showing first 1000 characters]{reset_color}\")\n",
        "                else:\n",
        "                    print(cleaned_output)\n",
        "            else:\n",
        "                print(f\"‚ùå ERROR: {output}\")\n",
        "\n",
        "            print(f\"{color}{'='*60}{reset_color}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def display_complete_responses_text(conversation_log: List[Dict]):\n",
        "        \"\"\"Display complete agent responses in clean text format for easy reading\"\"\"\n",
        "        print(\"\\n\" + \"üìñ\" * 60)\n",
        "        print(\"üìñ COMPLETE AGENT RESPONSES - FULL TEXT\")\n",
        "        print(\"üìñ\" * 60)\n",
        "\n",
        "    @staticmethod\n",
        "    def display_final_synthesis(conversation_log: List[Dict]):\n",
        "        \"\"\"Highlight the final output (last successful agent response)\"\"\"\n",
        "        final_entry = None\n",
        "        for entry in reversed(conversation_log):\n",
        "            if entry[\"success\"]:\n",
        "                final_entry = entry\n",
        "                break\n",
        "\n",
        "        if final_entry:\n",
        "            print(\"\\n\" + \"üéØ\" * 30)\n",
        "            print(\"üéØ FINAL PIPELINE RESULT\")\n",
        "            print(\"üéØ\" * 30)\n",
        "            print(f\"\\nFinal Agent: {final_entry['agent']} ({final_entry['role']})\")\n",
        "            print(f\"Model: {final_entry.get('model', 'Unknown')}\")\n",
        "            print(\"\\n\" + \"-\" * 80)\n",
        "            print(final_entry[\"output\"])\n",
        "            print(\"-\" * 80)\n",
        "            print(\"üéØ\" * 30)\n",
        "        else:\n",
        "            print(\"\\n‚ùå No successful agent responses found in conversation log\")\n",
        "\n",
        "        for entry in conversation_log:\n",
        "            step_num = entry[\"step\"]\n",
        "            agent_id = entry[\"agent\"]\n",
        "            role = entry[\"role\"]\n",
        "            model = entry.get(\"model\", \"Unknown\")\n",
        "            task = entry.get(\"task\", \"Analysis and processing\")\n",
        "            output = entry[\"output\"]\n",
        "            success = entry[\"success\"]\n",
        "            response_time = entry.get(\"response_time\", 0)\n",
        "\n",
        "            print(f\"\\n{'='*100}\")\n",
        "            print(f\"STEP {step_num}: {agent_id.upper()}\")\n",
        "            print(f\"Role: {role}\")\n",
        "            print(f\"Model: {model}\")\n",
        "            print(f\"Task: {task}\")\n",
        "            print(f\"Response Time: {response_time:.2f}s\")\n",
        "            print(f\"Status: {'‚úÖ Success' if success else '‚ùå Error'}\")\n",
        "            print(f\"{'='*100}\")\n",
        "\n",
        "            if success:\n",
        "                print(f\"\\nCOMPLETE RESPONSE:\")\n",
        "                print(f\"{'-'*100}\")\n",
        "                print(output)\n",
        "                print(f\"{'-'*100}\")\n",
        "            else:\n",
        "                print(f\"\\n‚ùå ERROR: {output}\")\n",
        "\n",
        "            print()  # Extra space between agents\n",
        "\n",
        "        print(\"üìñ\" * 60)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_conversation_table(conversation_log: List[Dict]) -> go.Figure:\n",
        "        \"\"\"Create comprehensive table showing complete agent responses\"\"\"\n",
        "        if not conversation_log:\n",
        "            return go.Figure().add_annotation(text=\"No conversation data available\")\n",
        "\n",
        "        # Prepare table data with COMPLETE responses\n",
        "        steps = []\n",
        "        agents = []\n",
        "        roles = []\n",
        "        models = []\n",
        "        tasks = []\n",
        "        responses = []\n",
        "        status = []\n",
        "        response_times = []\n",
        "\n",
        "        for entry in conversation_log:\n",
        "            steps.append(f\"Step {entry['step']}\")\n",
        "            agents.append(entry['agent'])\n",
        "            roles.append(entry['role'])\n",
        "            models.append(entry.get('model', 'Unknown'))\n",
        "\n",
        "            # Get complete task description\n",
        "            task_desc = entry.get('task', 'Analysis and processing')\n",
        "            tasks.append(task_desc)\n",
        "\n",
        "            # COMPLETE response text - NO TRUNCATION\n",
        "            response_text = entry['output']\n",
        "            responses.append(response_text)\n",
        "\n",
        "            status.append(\"‚úÖ Success\" if entry['success'] else \"‚ùå Error\")\n",
        "            response_times.append(f\"{entry.get('response_time', 0):.2f}s\")\n",
        "\n",
        "        # Create comprehensive table with complete responses\n",
        "        fig = go.Figure(data=[go.Table(\n",
        "            columnwidth=[60, 120, 150, 120, 150, 800, 80, 100],  # Wider response column\n",
        "            header=dict(\n",
        "                values=[\n",
        "                    '<b>Step</b>',\n",
        "                    '<b>Agent ID</b>',\n",
        "                    '<b>Role</b>',\n",
        "                    '<b>Model</b>',\n",
        "                    '<b>Task</b>',\n",
        "                    '<b>Complete Response</b>',\n",
        "                    '<b>Status</b>',\n",
        "                    '<b>Time</b>'\n",
        "                ],\n",
        "                fill_color='lightblue',\n",
        "                align='left',\n",
        "                font=dict(size=12, color='black'),\n",
        "                height=50\n",
        "            ),\n",
        "            cells=dict(\n",
        "                values=[steps, agents, roles, models, tasks, responses, status, response_times],\n",
        "                fill_color=[['white', 'lightgray'] * len(steps)],\n",
        "                align='left',\n",
        "                font=dict(size=11),\n",
        "                height=200,  # Taller cells to accommodate full responses\n",
        "                line=dict(color='darkslategray', width=1)\n",
        "            )\n",
        "        )])\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=f\"Complete Agent Pipeline Responses - {len(conversation_log)} Processing Steps\",\n",
        "            height=max(600, len(conversation_log) * 220 + 200),  # Dynamic height for readability\n",
        "            margin=dict(l=20, r=20, t=80, b=20),\n",
        "            font=dict(family=\"Arial, sans-serif\")\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def create_conversation_timeline(conversation_log: List[Dict]) -> go.Figure:\n",
        "        \"\"\"Create clear step-by-step conversation timeline for any number of agents\"\"\"\n",
        "        if not conversation_log:\n",
        "            return go.Figure().add_annotation(text=\"No conversation data available\")\n",
        "\n",
        "        # Extract data from conversation log\n",
        "        steps = [entry['step'] for entry in conversation_log]\n",
        "        agents = [entry['agent'] for entry in conversation_log]\n",
        "        roles = [entry['role'] for entry in conversation_log]\n",
        "        outputs = [entry['output'][:150] + \"...\" if len(entry['output']) > 150 else entry['output'] for entry in conversation_log]\n",
        "        success_status = [\"Success\" if entry['success'] else \"Error\" for entry in conversation_log]\n",
        "\n",
        "        # Create agent-to-index mapping for consistent coloring\n",
        "        unique_agents = []\n",
        "        for agent in agents:\n",
        "            if agent not in unique_agents:\n",
        "                unique_agents.append(agent)\n",
        "\n",
        "        # Generate colors for each agent\n",
        "        colors = []\n",
        "        for agent in agents:\n",
        "            agent_index = unique_agents.index(agent)\n",
        "            colors.append(CERTVisualizer.get_agent_color(agent_index, len(unique_agents)))\n",
        "\n",
        "        fig = go.Figure()\n",
        "\n",
        "        # Add scatter plot points for each step\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=steps,\n",
        "            y=[1] * len(steps),  # All on same horizontal line\n",
        "            mode='markers+text',\n",
        "            marker=dict(\n",
        "                size=25,\n",
        "                color=colors,\n",
        "                line=dict(width=2, color='white'),\n",
        "                opacity=0.8\n",
        "            ),\n",
        "            text=[f\"Step {step}<br>{agent}\" for step, agent in zip(steps, agents)],\n",
        "            textposition=\"top center\",\n",
        "            hovertemplate=\"<b>Step %{x}</b><br>Agent: %{customdata[0]}<br>Role: %{customdata[1]}<br>Status: %{customdata[2]}<br>Output: %{customdata[3]}<extra></extra>\",\n",
        "            customdata=list(zip(agents, roles, success_status, outputs)),\n",
        "            name=\"Processing Pipeline\"\n",
        "        ))\n",
        "\n",
        "        # Add connecting lines between steps\n",
        "        if len(steps) > 1:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=steps,\n",
        "                y=[1] * len(steps),\n",
        "                mode='lines',\n",
        "                line=dict(color='gray', width=3, dash='dash'),\n",
        "                showlegend=False,\n",
        "                hoverinfo='skip'\n",
        "            ))\n",
        "\n",
        "        # Add agent legend\n",
        "        for i, agent in enumerate(unique_agents):\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=[None], y=[None],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=15,\n",
        "                    color=CERTVisualizer.get_agent_color(i, len(unique_agents))\n",
        "                ),\n",
        "                name=agent,\n",
        "                showlegend=True\n",
        "            ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=f\"Agent Processing Pipeline - {len(unique_agents)} Agents, {len(steps)} Sequential Steps\",\n",
        "            xaxis_title=\"Processing Step\",\n",
        "            yaxis=dict(visible=False),  # Hide y-axis since all points are on same line\n",
        "            height=400,\n",
        "            xaxis=dict(\n",
        "                tickmode='linear',\n",
        "                tick0=1,\n",
        "                dtick=1,\n",
        "                range=[0.5, max(steps) + 0.5]\n",
        "            ),\n",
        "            legend=dict(\n",
        "                orientation=\"h\",\n",
        "                yanchor=\"bottom\",\n",
        "                y=1.02,\n",
        "                xanchor=\"right\",\n",
        "                x=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def create_performance_dashboard(agents: List[ClaudeAgent],\n",
        "                                   coordination_effects: List[Dict]) -> go.Figure:\n",
        "        \"\"\"Create comprehensive performance dashboard\"\"\"\n",
        "\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Agent Consistency Scores (Œ≤)\",\n",
        "                \"Response Times\",\n",
        "                \"Coordination Effects (Œ≥)\",\n",
        "                \"Success Rates\"\n",
        "            ),\n",
        "            specs=[\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Consistency scores\n",
        "        agent_names = [agent.agent_id for agent in agents]\n",
        "        consistency_scores = []\n",
        "\n",
        "        for agent in agents:\n",
        "            if agent.performance_metrics[\"consistency_scores\"]:\n",
        "                consistency_scores.append(np.mean(agent.performance_metrics[\"consistency_scores\"]))\n",
        "            else:\n",
        "                consistency_scores.append(0.0)\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=agent_names,\n",
        "                y=consistency_scores,\n",
        "                name=\"Consistency (Œ≤)\",\n",
        "                marker=dict(color=consistency_scores, colorscale=\"Viridis\")\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Response times\n",
        "        for agent in agents:\n",
        "            if agent.performance_metrics[\"response_times\"]:\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=list(range(len(agent.performance_metrics[\"response_times\"]))),\n",
        "                        y=agent.performance_metrics[\"response_times\"],\n",
        "                        mode=\"lines+markers\",\n",
        "                        name=f\"{agent.agent_id} Response Time\"\n",
        "                    ),\n",
        "                    row=1, col=2\n",
        "                )\n",
        "\n",
        "        # Coordination effects\n",
        "        if coordination_effects:\n",
        "            gammas = [ce[\"coordination_effect\"] for ce in coordination_effects]\n",
        "            timestamps = [ce[\"timestamp\"] for ce in coordination_effects]\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=timestamps,\n",
        "                    y=gammas,\n",
        "                    mode=\"lines+markers\",\n",
        "                    name=\"Coordination Effect (Œ≥)\",\n",
        "                    line=dict(color=\"red\")\n",
        "                ),\n",
        "                row=2, col=1\n",
        "            )\n",
        "\n",
        "        # Success rates\n",
        "        success_rates = []\n",
        "        for agent in agents:\n",
        "            total = agent.performance_metrics[\"success_count\"] + agent.performance_metrics[\"error_count\"]\n",
        "            if total > 0:\n",
        "                success_rates.append(agent.performance_metrics[\"success_count\"] / total)\n",
        "            else:\n",
        "                success_rates.append(0.0)\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=agent_names,\n",
        "                y=success_rates,\n",
        "                name=\"Success Rate\",\n",
        "                marker=dict(color=success_rates, colorscale=\"RdYlGn\")\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        fig.update_layout(height=800, title_text=\"CERT Agents Pipeline Performance Dashboard\")\n",
        "        return fig\n",
        "\n",
        "class PDFProcessor:\n",
        "    \"\"\"Handles PDF upload and text extraction\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def upload_and_extract() -> Dict[str, str]:\n",
        "        \"\"\"Upload PDF and extract text content\"\"\"\n",
        "        print(\"üìÑ Upload PDF documents for analysis...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        documents = {}\n",
        "        for filename, content in uploaded.items():\n",
        "            if filename.endswith('.pdf'):\n",
        "                try:\n",
        "                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))\n",
        "                    text = \"\"\n",
        "                    for page in pdf_reader.pages:\n",
        "                        text += page.extract_text() + \"\\n\"\n",
        "\n",
        "                    documents[filename] = text\n",
        "                    print(f\"‚úÖ Extracted {len(text)} characters from {filename}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error processing {filename}: {str(e)}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Skipping {filename} (not a PDF)\")\n",
        "\n",
        "        return documents\n",
        "\n",
        "# Available Claude Models\n",
        "CLAUDE_MODELS = {\n",
        "    \"claude-opus-4-20250514\": \"Claude Opus 4 (Latest, Most Powerful)\",\n",
        "    \"claude-sonnet-4-20250514\": \"Claude Sonnet 4 (Latest, Balanced)\",\n",
        "    \"claude-3-5-haiku-20241022\": \"Claude 3.5 Haiku (Fast)\",\n",
        "    \"claude-3-7-sonnet-20250219\": \"Claude 3.7 Sonnet\",\n",
        "    \"claude-3-5-sonnet-20241022\": \"Claude 3.5 Sonnet\",\n",
        "    \"claude-3-5-sonnet-20240620\": \"Claude 3.5 Sonnet (June)\",\n",
        "    \"claude-3-haiku-20240307\": \"Claude 3 Haiku (Legacy)\"\n",
        "}\n",
        "\n",
        "# Configuration Interface\n",
        "def create_agent_config_interface(max_agents: int = 10):\n",
        "    \"\"\"Create interactive interface for configuring agents\"\"\"\n",
        "\n",
        "    def create_agent_widgets(agent_num: int):\n",
        "        agent_id = widgets.Text(\n",
        "            value=f\"agent_{agent_num}\",\n",
        "            description=f\"Agent {agent_num} ID:\",\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        model = widgets.Dropdown(\n",
        "            options=list(CLAUDE_MODELS.keys()),\n",
        "            value=\"claude-sonnet-4-20250514\",\n",
        "            description=f\"Model:\",\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        role = widgets.Text(\n",
        "            value=f\"Analyst {agent_num}\",\n",
        "            description=f\"Role:\",\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        task = widgets.Textarea(\n",
        "            value=f\"Analyze documents and provide insights based on your specialized perspective\",\n",
        "            description=f\"Task:\",\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"id\": agent_id,\n",
        "            \"model\": model,\n",
        "            \"role\": role,\n",
        "            \"task\": task\n",
        "        }\n",
        "\n",
        "    # Number of agents selector\n",
        "    num_agents = widgets.IntSlider(\n",
        "        value=3,\n",
        "        min=2,\n",
        "        max=max_agents,\n",
        "        description=\"Number of Agents:\",\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    # Global task configuration\n",
        "    global_task = widgets.Textarea(\n",
        "        value=\"Analyze the uploaded PDF document and provide comprehensive insights\",\n",
        "        description=\"Global Task:\",\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    coordination_pattern = widgets.Dropdown(\n",
        "        options=[\"sequential\", \"parallel\"],\n",
        "        value=\"sequential\",\n",
        "        description=\"Coordination Pattern:\",\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"num_agents\": num_agents,\n",
        "        \"global_task\": global_task,\n",
        "        \"coordination_pattern\": coordination_pattern,\n",
        "        \"create_agent_widgets\": create_agent_widgets\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnQZuDMDXdDD"
      },
      "outputs": [],
      "source": [
        "async def run_claude_cert_demo(\n",
        "    api_key: str,\n",
        "    agent_configs: List[Dict[str, str]],\n",
        "    global_task: str,\n",
        "    coordination_pattern: str = \"sequential\",\n",
        "    consistency_trials: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Run the complete Claude-only CERT coordination demonstration\n",
        "\n",
        "    Args:\n",
        "        api_key: Anthropic API key\n",
        "        agent_configs: List of agent configurations with id, model, role, task\n",
        "        global_task: Overall coordination task\n",
        "        coordination_pattern: \"sequential\" or \"parallel\"\n",
        "        consistency_trials: Number of trials for consistency measurement\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ Claude-Only CERT Agents Pipeline Coordination Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Agents: {len(agent_configs)}\")\n",
        "    print(f\"Pattern: {coordination_pattern}\")\n",
        "    print(f\"Task: {global_task}\")\n",
        "    print()\n",
        "\n",
        "    # Create agents\n",
        "    agents = []\n",
        "    for config in agent_configs:\n",
        "        agent = ClaudeAgent(\n",
        "            agent_id=config[\"id\"],\n",
        "            model=config[\"model\"],\n",
        "            role=config[\"role\"],\n",
        "            task_description=config[\"task\"],\n",
        "            api_key=api_key\n",
        "        )\n",
        "        agents.append(agent)\n",
        "        print(f\"‚úÖ Created {config['id']} using {CLAUDE_MODELS[config['model']]}\")\n",
        "\n",
        "    # Upload and process documents\n",
        "    documents = PDFProcessor.upload_and_extract()\n",
        "\n",
        "    if not documents:\n",
        "        print(\"‚ùå No documents uploaded\")\n",
        "        return None\n",
        "\n",
        "    # Take first document for analysis\n",
        "    doc_name, doc_content = list(documents.items())[0]\n",
        "    print(f\"\\nüìÑ Analyzing: {doc_name}\")\n",
        "\n",
        "    # Phase 1: Individual agent consistency measurement\n",
        "    print(\"\\nüîç Phase 1: Individual Agent Analysis\")\n",
        "    for agent in agents:\n",
        "        print(f\"  Testing {agent.agent_id}...\")\n",
        "        consistency = await agent.measure_consistency(global_task, consistency_trials)\n",
        "        print(f\"    Consistency (Œ≤): {consistency:.3f}\")\n",
        "\n",
        "    # Phase 2: Coordination measurement\n",
        "    print(f\"\\nü§ù Phase 2: Agents Pipeline Coordination ({coordination_pattern})\")\n",
        "\n",
        "    orchestrator = CoordinationOrchestrator(agents)\n",
        "\n",
        "    if coordination_pattern == \"sequential\":\n",
        "        coordination_steps = await orchestrator.run_sequential_coordination(global_task, doc_content)\n",
        "    else:\n",
        "        coordination_steps = await orchestrator.run_parallel_coordination(global_task, doc_content)\n",
        "\n",
        "    # Calculate coordination effect\n",
        "    gamma = orchestrator.measure_coordination_effect(coordination_steps)\n",
        "    print(f\"Coordination Effect (Œ≥): {gamma:.3f}\")\n",
        "\n",
        "    # Phase 3: Generate visualizations\n",
        "    print(\"\\nüìä Phase 3: Generating Results & Visualizations\")\n",
        "\n",
        "    # Display complete responses in clean text format\n",
        "    CERTVisualizer.display_complete_responses_text(orchestrator.conversation_log)\n",
        "\n",
        "    # Highlight the final result\n",
        "    CERTVisualizer.display_final_synthesis(orchestrator.conversation_log)\n",
        "\n",
        "    # Create interactive table for complete responses\n",
        "    conversation_table = CERTVisualizer.create_conversation_table(orchestrator.conversation_log)\n",
        "    dashboard_fig = CERTVisualizer.create_performance_dashboard(agents, orchestrator.coordination_effects)\n",
        "\n",
        "    # Display results\n",
        "    display(HTML(\"<h2>üéØ CERT Claude-Only Pipeline Analysis Results</h2>\"))\n",
        "    display(HTML(f\"<p><b>Document:</b> {doc_name}</p>\"))\n",
        "    display(HTML(f\"<p><b>Processing Pipeline:</b> {len(agents)} agents in {coordination_pattern} mode</p>\"))\n",
        "\n",
        "    print(\"\\nüìã INTERACTIVE COMPLETE RESPONSES TABLE:\")\n",
        "    print(\"üìñ Scrollable table with full agent responses - no truncation\")\n",
        "    display(conversation_table)\n",
        "\n",
        "    print(\"\\nüìä Performance Metrics Dashboard:\")\n",
        "    display(dashboard_fig)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\nüìã Summary Statistics:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    for agent in agents:\n",
        "        if agent.performance_metrics[\"consistency_scores\"]:\n",
        "            avg_consistency = np.mean(agent.performance_metrics[\"consistency_scores\"])\n",
        "            avg_response_time = np.mean(agent.performance_metrics[\"response_times\"])\n",
        "            success_rate = agent.performance_metrics[\"success_count\"] / (\n",
        "                agent.performance_metrics[\"success_count\"] + agent.performance_metrics[\"error_count\"]\n",
        "            )\n",
        "\n",
        "            print(f\"{agent.agent_id}:\")\n",
        "            print(f\"  ‚Ä¢ Model: {CLAUDE_MODELS[agent.model]}\")\n",
        "            print(f\"  ‚Ä¢ Consistency (Œ≤): {avg_consistency:.3f}\")\n",
        "            print(f\"  ‚Ä¢ Avg Response Time: {avg_response_time:.2f}s\")\n",
        "            print(f\"  ‚Ä¢ Success Rate: {success_rate:.2%}\")\n",
        "\n",
        "    print(f\"\\nOverall Coordination Effect (Œ≥): {gamma:.3f}\")\n",
        "\n",
        "    # Enhanced coordination analysis\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üìä PROCESSING PIPELINE ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if gamma > 1.2:\n",
        "        print(\"üöÄ STRONG PIPELINE BENEFIT - Sequential processing significantly improves output quality\")\n",
        "        print(f\"   Pipeline improvement: {(gamma-1)*100:.1f}% above individual agent baseline\")\n",
        "    elif gamma > 1.0:\n",
        "        print(\"‚úÖ POSITIVE PIPELINE EFFECT - Sequential processing improves output quality\")\n",
        "        print(f\"   Pipeline improvement: {(gamma-1)*100:.1f}% above individual agent baseline\")\n",
        "    elif gamma > 0.8:\n",
        "        print(\"‚û°Ô∏è NEUTRAL PIPELINE EFFECT - Minimal benefit from sequential processing\")\n",
        "        print(f\"   Pipeline change: {(gamma-1)*100:.1f}% from individual baseline\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è NEGATIVE PIPELINE EFFECT - Sequential processing may degrade quality\")\n",
        "        print(f\"   Pipeline degradation: {(1-gamma)*100:.1f}% below individual baseline\")\n",
        "\n",
        "    print(f\"\\nüí° SCIENTIFIC INTERPRETATION:\")\n",
        "    print(f\"This measures coordination effects in a predetermined processing pipeline where\")\n",
        "    print(f\"each Claude instance processes the output of the previous one. This is valuable\")\n",
        "    print(f\"infrastructure for current token-manipulation systems, not genuine agentic AI.\")\n",
        "\n",
        "    # Agent performance ranking\n",
        "    agent_rankings = []\n",
        "    for agent in agents:\n",
        "        if agent.performance_metrics[\"consistency_scores\"]:\n",
        "            avg_consistency = np.mean(agent.performance_metrics[\"consistency_scores\"])\n",
        "            agent_rankings.append((agent.agent_id, avg_consistency, agent.model))\n",
        "\n",
        "    agent_rankings.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(f\"\\nüèÜ AGENT PERFORMANCE RANKING:\")\n",
        "    for i, (agent_id, consistency, model) in enumerate(agent_rankings):\n",
        "        print(f\"   {i+1}. {agent_id}: Œ≤={consistency:.3f} ({CLAUDE_MODELS[model]})\")\n",
        "\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Return complete results\n",
        "    return {\n",
        "        \"agents\": agents,\n",
        "        \"orchestrator\": orchestrator,\n",
        "        \"coordination_steps\": coordination_steps,\n",
        "        \"coordination_effect\": gamma,\n",
        "        \"conversation_log\": orchestrator.conversation_log,\n",
        "        \"documents\": documents\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGGWRtm4bbPS"
      },
      "source": [
        "##¬†Run the Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awmM8BKlc4ju"
      },
      "source": [
        "**Example Configuration**\n",
        "```\n",
        "agent_configs = [\n",
        "    {\n",
        "        \"id\": \"primary_analyst\",\n",
        "        \"model\": \"claude-sonnet-4-20250514\",\n",
        "        \"role\": \"Primary Document Analyst\",\n",
        "        \"task\": \"Extract key themes and main arguments from documents\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"critical_reviewer\",\n",
        "        \"model\": \"claude-3-5-haiku-20241022\",\n",
        "        \"role\": \"Critical Reviewer\",\n",
        "        \"task\": \"Identify gaps, contradictions, and areas needing clarification\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"synthesizer\",\n",
        "        \"model\": \"claude-opus-4-20250514\",\n",
        "        \"role\": \"Synthesis Specialist\",\n",
        "        \"task\": \"Integrate multiple perspectives into coherent conclusions\"\n",
        "    }\n",
        "]\n",
        "\n",
        "global_task = \"Analyze the uploaded document for strategic insights and actionable recommendations\"\n",
        "coordination_pattern = \"sequential\"  # or \"parallel\"\n",
        "consistency_trials = 3\n",
        "\n",
        "results = await run_claude_cert_demo(\n",
        "    api_key=anthropic_api_key,\n",
        "    agent_configs=agent_configs,\n",
        "    global_task=global_task,\n",
        "    coordination_pattern=coordination_pattern,\n",
        "    consistency_trials=consistency_trials\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roC9CQNOgtC-"
      },
      "outputs": [],
      "source": [
        "agent_configs = [\n",
        "    {\n",
        "        \"id\": \"primary_analyst\",\n",
        "        \"model\": \"claude-sonnet-4-20250514\",\n",
        "        \"role\": \"Primary Document Analyst\",\n",
        "        \"task\": \"Extract key themes and main arguments from documents\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"critical_reviewer\",\n",
        "        \"model\": \"claude-3-5-haiku-20241022\",\n",
        "        \"role\": \"Critical Reviewer\",\n",
        "        \"task\": \"Identify gaps, contradictions, and areas needing clarification\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"synthesizer\",\n",
        "        \"model\": \"claude-opus-4-20250514\",\n",
        "        \"role\": \"Synthesis Specialist\",\n",
        "        \"task\": \"Integrate multiple perspectives into coherent conclusions\"\n",
        "    }\n",
        "]\n",
        "\n",
        "global_task = \"List three main points from this document\"\n",
        "coordination_pattern = \"sequential\"  # or \"parallel\"\n",
        "consistency_trials = 3\n",
        "\n",
        "results = await run_claude_cert_demo(\n",
        "    api_key=ANTHROPIC_API_KEY,\n",
        "    agent_configs=agent_configs,\n",
        "    global_task=global_task,\n",
        "    coordination_pattern=coordination_pattern,\n",
        "    consistency_trials=consistency_trials\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
