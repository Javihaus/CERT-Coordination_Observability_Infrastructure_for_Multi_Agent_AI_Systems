{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW8UKTx1YNjr"
      },
      "source": [
        "##**Claude-Only CERT Multi-Agent Coordination Demo**\n",
        "---\n",
        "\n",
        "This framework provides systematic measurement of coordination patterns in current LLM systems. While these systems manipulate discrete tokens based on statistical correlations rather than learning continuous representations of the physical world, this infrastructure will be essential when we develop architectures based on learned world models.\n",
        "\n",
        "**What this measures:**\n",
        "Coordination behaviors between sophisticated pattern-matching systems.\n",
        "\n",
        "**What this enables:**\n",
        "Deployment scaffolding for current technology + experimental apparatus for studying genuine coordination when it emerges from proper architectures.\n",
        "\n",
        "###**Setup Instructions**\n",
        "####1. API Configuration\n",
        "python# Set your Anthropic API key\n",
        "api_key = \"sk-ant-api03-your-key-here\"\n",
        "####2. Agent Configuration (2-10 agents)\n",
        "Each agent requires four parameters:\n",
        "\n",
        ">Agent ID: Unique identifier (agent_1, summarizer, etc.)\n",
        "\n",
        ">Model: Choose from available Claude models\n",
        "\n",
        ">Role: Agent's specialized function (Document Analyst, Critical Reviewer)\n",
        "\n",
        ">Task: Specific instructions for this agent's analysis approach\n",
        "\n",
        "Available Models:\n",
        "```\n",
        "claude-opus-4-20250514 - #Most powerful, complex reasoning\n",
        "claude-sonnet-4-20250514 - #Balanced performance (recommended)\n",
        "claude-3-5-haiku-20241022 - #Fastest response times\n",
        "claude-3-haiku-20240307 - #Legacy model for comparison\n",
        "```\n",
        "####3. Coordination Configuration\n",
        "Global Task: Overall objective for the multi-agent system\n",
        "\n",
        "Coordination Pattern:\n",
        ">Sequential: Agents process in order, building on previous outputs\n",
        "\n",
        ">Parallel: All agents analyze simultaneously, independent perspectives\n",
        "\n",
        "####4. Document Upload\n",
        "Upload PDF documents for analysis. The system extracts text content and uses it as context for agent coordination.\n",
        "\n",
        "###**Execution Process**\n",
        "\n",
        "####Phase 1: Individual Analysis\n",
        "\n",
        ">**Behavioral Consistency Score ($C$)**\n",
        "\n",
        "\n",
        ">How reliably an agent produces similar responses to identical tasks.\n",
        "\n",
        ">for $C$ in the range 0.9-1.0: Highly reliable\n",
        "\n",
        ">for $C$ in the range 0.7-0.9: Moderately consistent\n",
        "\n",
        ">for $C$<0.7: Unreliable for deployment\n",
        "\n",
        "\n",
        "####Phase 2: Multi-Agent Coordination\n",
        "Agents coordinate according to selected pattern while system tracks:\n",
        "\n",
        ">Conversation flow: Complete interaction sequence\n",
        "\n",
        ">Response quality: Success/failure rates\n",
        "\n",
        ">Timing patterns: Response latencies and bottlenecks\n",
        "\n",
        "####Phase 3: Coordination Effect Measurement\n",
        ">**Coordination Effect ($\\gamma$)**\n",
        "\n",
        ">Performance change when agents work together vs. alone.\n",
        "\n",
        "<center>$\\gamma = \\frac{\\textrm{Coordinated Performance}}{\\textrm{Individual Performance}}$</center>\n",
        "\n",
        "> $\\gamma$ > 1.0: Agents help each other\n",
        "\n",
        "> $\\gamma$ = 1.0: No coordination benefit\n",
        "\n",
        "> $\\gamma$ < 1.0: Agents interfere with each other\n",
        "\n",
        "###**Interactive Visualization**\n",
        "Conversation Timeline: Real-time tracking of agent interactions with step-by-step conversation flow\n",
        "performance four-panel analysis showing:\n",
        "\n",
        "> Agent consistency scores over time\n",
        "\n",
        "> Response time patterns by agent\n",
        "\n",
        "> Coordination effects across experiments\n",
        "\n",
        "> Success rates and error analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzbede_Gbt_d"
      },
      "source": [
        "## Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZgzcI_STnuT"
      },
      "outputs": [],
      "source": [
        "##Install required packages and clone the CERT repository##\n",
        "#!pip install anthropic transformers torch dotenv pycryptodome PyPDF2\n",
        "#!pip install -q watermark\n",
        "## Clone CERT repository##\n",
        "#!git clone https://github.com/Javihaus/cert-coordination-observability.git\n",
        "#!cd cert-coordination-observability && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7-dyLTydsUh"
      },
      "outputs": [],
      "source": [
        "%load_ext watermark\n",
        "%watermark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GnRnjaJeCXZ"
      },
      "source": [
        "**Test environment**\n",
        "\n",
        "Python implementation: CPython <br>\n",
        "Python version       : 3.11.13<br>\n",
        "IPython version      : 7.34.0<br>\n",
        "\n",
        "Compiler    : GCC 11.4.0<br>\n",
        "OS          : Linux<br>\n",
        "Release     : 6.1.123+<br>\n",
        "Machine     : x86_64<br>\n",
        "Processor   : x86_64<br>\n",
        "CPU cores   : 2<br>\n",
        "Architecture: 64bit<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARyVt-NoXQk_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import anthropic\n",
        "import PyPDF2\n",
        "import io\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPZsk67bXZqd"
      },
      "outputs": [],
      "source": [
        "ANTHROPIC_API_KEY=\"xxxxxxxxxxx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YZQloAzddQt"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oscq0PhLXdGI"
      },
      "outputs": [],
      "source": [
        "# CERT Core Measurement Components\n",
        "@dataclass\n",
        "class AgentInteraction:\n",
        "    timestamp: datetime\n",
        "    agent_id: str\n",
        "    model: str\n",
        "    role: str\n",
        "    task: str\n",
        "    prompt: str\n",
        "    response: str\n",
        "    response_time: float\n",
        "    success: bool\n",
        "    error: Optional[str] = None\n",
        "    metadata: Dict[str, Any] = None\n",
        "\n",
        "@dataclass\n",
        "class CoordinationStep:\n",
        "    step_number: int\n",
        "    agent_id: str\n",
        "    input_context: str\n",
        "    output: str\n",
        "    reasoning: str\n",
        "    timestamp: datetime\n",
        "\n",
        "class CERTMeasurement:\n",
        "    \"\"\"Core measurement logic for behavioral consistency and coordination effects\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_behavioral_consistency(responses: List[str]) -> float:\n",
        "        \"\"\"Calculate consistency score (β) from multiple responses\"\"\"\n",
        "        if len(responses) < 2:\n",
        "            return 1.0\n",
        "\n",
        "        # Simple token-based consistency measurement\n",
        "        token_sets = [set(response.lower().split()) for response in responses]\n",
        "\n",
        "        similarities = []\n",
        "        for i in range(len(token_sets)):\n",
        "            for j in range(i + 1, len(token_sets)):\n",
        "                intersection = len(token_sets[i].intersection(token_sets[j]))\n",
        "                union = len(token_sets[i].union(token_sets[j]))\n",
        "                similarity = intersection / union if union > 0 else 0\n",
        "                similarities.append(similarity)\n",
        "\n",
        "        return np.mean(similarities) if similarities else 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_coordination_effect(individual_performances: List[float],\n",
        "                                    coordinated_performance: float) -> float:\n",
        "        \"\"\"Calculate coordination effect (γ)\"\"\"\n",
        "        expected_performance = np.mean(individual_performances)\n",
        "        if expected_performance == 0:\n",
        "            return 1.0\n",
        "        return coordinated_performance / expected_performance\n",
        "\n",
        "class ClaudeAgent:\n",
        "    \"\"\"Individual Claude agent with specific role and model\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str, model: str, role: str, task_description: str, api_key: str):\n",
        "        self.agent_id = agent_id\n",
        "        self.model = model\n",
        "        self.role = role\n",
        "        self.task_description = task_description\n",
        "        self.client = anthropic.Anthropic(api_key=api_key)\n",
        "        self.interaction_history = []\n",
        "        self.performance_metrics = {\n",
        "            \"consistency_scores\": [],\n",
        "            \"response_times\": [],\n",
        "            \"error_count\": 0,\n",
        "            \"success_count\": 0\n",
        "        }\n",
        "\n",
        "    async def generate_response(self, prompt: str, context: str = \"\",\n",
        "                              max_tokens: int = 500) -> AgentInteraction:\n",
        "        \"\"\"Generate response and track performance metrics\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            full_prompt = f\"\"\"Role: {self.role}\n",
        "Task: {self.task_description}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Request: {prompt}\n",
        "\n",
        "Respond according to your role and task. Be concise but thorough.\"\"\"\n",
        "\n",
        "            response = self.client.messages.create(\n",
        "                model=self.model,\n",
        "                max_tokens=max_tokens,\n",
        "                messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
        "            )\n",
        "\n",
        "            response_time = time.time() - start_time\n",
        "            response_text = response.content[0].text\n",
        "\n",
        "            interaction = AgentInteraction(\n",
        "                timestamp=datetime.now(),\n",
        "                agent_id=self.agent_id,\n",
        "                model=self.model,\n",
        "                role=self.role,\n",
        "                task=self.task_description,\n",
        "                prompt=prompt,\n",
        "                response=response_text,\n",
        "                response_time=response_time,\n",
        "                success=True,\n",
        "                metadata={\"context\": context, \"max_tokens\": max_tokens}\n",
        "            )\n",
        "\n",
        "            self.interaction_history.append(interaction)\n",
        "            self.performance_metrics[\"success_count\"] += 1\n",
        "            self.performance_metrics[\"response_times\"].append(response_time)\n",
        "\n",
        "            return interaction\n",
        "\n",
        "        except Exception as e:\n",
        "            response_time = time.time() - start_time\n",
        "\n",
        "            interaction = AgentInteraction(\n",
        "                timestamp=datetime.now(),\n",
        "                agent_id=self.agent_id,\n",
        "                model=self.model,\n",
        "                role=self.role,\n",
        "                task=self.task_description,\n",
        "                prompt=prompt,\n",
        "                response=\"\",\n",
        "                response_time=response_time,\n",
        "                success=False,\n",
        "                error=str(e)\n",
        "            )\n",
        "\n",
        "            self.interaction_history.append(interaction)\n",
        "            self.performance_metrics[\"error_count\"] += 1\n",
        "\n",
        "            return interaction\n",
        "\n",
        "    async def measure_consistency(self, prompt: str, trials: int = 3) -> float:\n",
        "        \"\"\"Measure behavioral consistency across multiple trials\"\"\"\n",
        "        responses = []\n",
        "\n",
        "        for _ in range(trials):\n",
        "            interaction = await self.generate_response(prompt)\n",
        "            if interaction.success:\n",
        "                responses.append(interaction.response)\n",
        "            await asyncio.sleep(0.5)  # Rate limiting\n",
        "\n",
        "        if len(responses) >= 2:\n",
        "            consistency = CERTMeasurement.calculate_behavioral_consistency(responses)\n",
        "            self.performance_metrics[\"consistency_scores\"].append(consistency)\n",
        "            return consistency\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "class CoordinationOrchestrator:\n",
        "    \"\"\"Manages multi-agent coordination and conversation tracking\"\"\"\n",
        "\n",
        "    def __init__(self, agents: List[ClaudeAgent]):\n",
        "        self.agents = agents\n",
        "        self.coordination_history = []\n",
        "        self.conversation_log = []\n",
        "        self.coordination_effects = []\n",
        "\n",
        "    async def run_sequential_coordination(self, initial_prompt: str,\n",
        "                                        document_content: str = \"\") -> List[CoordinationStep]:\n",
        "        \"\"\"Run sequential coordination between agents\"\"\"\n",
        "        coordination_steps = []\n",
        "        current_context = f\"Document: {document_content}\\n\\nInitial Task: {initial_prompt}\"\n",
        "\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            step_prompt = f\"Step {i+1}: {current_context}\"\n",
        "\n",
        "            interaction = await agent.generate_response(step_prompt, current_context)\n",
        "\n",
        "            step = CoordinationStep(\n",
        "                step_number=i + 1,\n",
        "                agent_id=agent.agent_id,\n",
        "                input_context=current_context,\n",
        "                output=interaction.response if interaction.success else f\"ERROR: {interaction.error}\",\n",
        "                reasoning=f\"Agent {agent.agent_id} ({agent.role}) processing step {i+1}\",\n",
        "                timestamp=interaction.timestamp\n",
        "            )\n",
        "\n",
        "            coordination_steps.append(step)\n",
        "            self.conversation_log.append({\n",
        "                \"step\": i + 1,\n",
        "                \"agent\": agent.agent_id,\n",
        "                \"role\": agent.role,\n",
        "                \"input\": current_context[:200] + \"...\" if len(current_context) > 200 else current_context,\n",
        "                \"output\": step.output,\n",
        "                \"timestamp\": step.timestamp,\n",
        "                \"success\": interaction.success\n",
        "            })\n",
        "\n",
        "            # Update context for next agent\n",
        "            if interaction.success:\n",
        "                current_context = f\"Previous analysis: {interaction.response}\\n\\nContinue the analysis:\"\n",
        "\n",
        "            await asyncio.sleep(1)  # Rate limiting between agents\n",
        "\n",
        "        self.coordination_history.extend(coordination_steps)\n",
        "        return coordination_steps\n",
        "\n",
        "    async def run_parallel_coordination(self, task_prompt: str,\n",
        "                                      document_content: str = \"\") -> List[CoordinationStep]:\n",
        "        \"\"\"Run parallel coordination where all agents work simultaneously\"\"\"\n",
        "        coordination_steps = []\n",
        "        context = f\"Document: {document_content}\\n\\nTask: {task_prompt}\"\n",
        "\n",
        "        # All agents process the same prompt simultaneously\n",
        "        tasks = []\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            tasks.append(agent.generate_response(task_prompt, context))\n",
        "\n",
        "        interactions = await asyncio.gather(*tasks)\n",
        "\n",
        "        for i, (agent, interaction) in enumerate(zip(self.agents, interactions)):\n",
        "            step = CoordinationStep(\n",
        "                step_number=i + 1,\n",
        "                agent_id=agent.agent_id,\n",
        "                input_context=context,\n",
        "                output=interaction.response if interaction.success else f\"ERROR: {interaction.error}\",\n",
        "                reasoning=f\"Agent {agent.agent_id} ({agent.role}) parallel processing\",\n",
        "                timestamp=interaction.timestamp\n",
        "            )\n",
        "\n",
        "            coordination_steps.append(step)\n",
        "            self.conversation_log.append({\n",
        "                \"step\": i + 1,\n",
        "                \"agent\": agent.agent_id,\n",
        "                \"role\": agent.role,\n",
        "                \"input\": context[:200] + \"...\" if len(context) > 200 else context,\n",
        "                \"output\": step.output,\n",
        "                \"timestamp\": step.timestamp,\n",
        "                \"success\": interaction.success\n",
        "            })\n",
        "\n",
        "        self.coordination_history.extend(coordination_steps)\n",
        "        return coordination_steps\n",
        "\n",
        "    def measure_coordination_effect(self, coordination_steps: List[CoordinationStep]) -> float:\n",
        "        \"\"\"Measure overall coordination effect\"\"\"\n",
        "        # Get individual baseline performances\n",
        "        individual_performances = []\n",
        "        for agent in self.agents:\n",
        "            if agent.performance_metrics[\"consistency_scores\"]:\n",
        "                individual_performances.append(np.mean(agent.performance_metrics[\"consistency_scores\"]))\n",
        "            else:\n",
        "                individual_performances.append(0.5)  # Default baseline\n",
        "\n",
        "        # Simulate coordinated performance based on successful steps\n",
        "        successful_steps = [step for step in coordination_steps if \"ERROR\" not in step.output]\n",
        "        coordinated_performance = len(successful_steps) / len(coordination_steps) if coordination_steps else 0\n",
        "\n",
        "        gamma = CERTMeasurement.calculate_coordination_effect(individual_performances, coordinated_performance)\n",
        "\n",
        "        self.coordination_effects.append({\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"individual_performances\": individual_performances,\n",
        "            \"coordinated_performance\": coordinated_performance,\n",
        "            \"coordination_effect\": gamma,\n",
        "            \"successful_steps\": len(successful_steps),\n",
        "            \"total_steps\": len(coordination_steps)\n",
        "        })\n",
        "\n",
        "        return gamma\n",
        "\n",
        "class CERTVisualizer:\n",
        "    \"\"\"Creates interactive visualizations of coordination behavior\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_conversation_timeline(conversation_log: List[Dict]) -> go.Figure:\n",
        "        \"\"\"Create interactive timeline of agent conversations\"\"\"\n",
        "        df = pd.DataFrame(conversation_log)\n",
        "\n",
        "        if df.empty:\n",
        "            return go.Figure().add_annotation(text=\"No conversation data available\")\n",
        "\n",
        "        fig = px.timeline(\n",
        "            df,\n",
        "            x_start=\"timestamp\",\n",
        "            x_end=\"timestamp\",\n",
        "            y=\"agent\",\n",
        "            color=\"role\",\n",
        "            title=\"Agent Conversation Timeline\",\n",
        "            hover_data=[\"step\", \"success\"]\n",
        "        )\n",
        "\n",
        "        # Add conversation content as annotations\n",
        "        for i, row in df.iterrows():\n",
        "            fig.add_annotation(\n",
        "                x=row[\"timestamp\"],\n",
        "                y=row[\"agent\"],\n",
        "                text=f\"Step {row['step']}: {row['output'][:50]}...\",\n",
        "                showarrow=True,\n",
        "                arrowhead=2\n",
        "            )\n",
        "\n",
        "        fig.update_layout(height=400)\n",
        "        return fig\n",
        "\n",
        "    @staticmethod\n",
        "    def create_performance_dashboard(agents: List[ClaudeAgent],\n",
        "                                   coordination_effects: List[Dict]) -> go.Figure:\n",
        "        \"\"\"Create comprehensive performance dashboard\"\"\"\n",
        "\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Agent Consistency Scores (β)\",\n",
        "                \"Response Times\",\n",
        "                \"Coordination Effects (γ)\",\n",
        "                \"Success Rates\"\n",
        "            ),\n",
        "            specs=[\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Consistency scores\n",
        "        agent_names = [agent.agent_id for agent in agents]\n",
        "        consistency_scores = []\n",
        "\n",
        "        for agent in agents:\n",
        "            if agent.performance_metrics[\"consistency_scores\"]:\n",
        "                consistency_scores.append(np.mean(agent.performance_metrics[\"consistency_scores\"]))\n",
        "            else:\n",
        "                consistency_scores.append(0.0)\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=agent_names,\n",
        "                y=consistency_scores,\n",
        "                name=\"Consistency (β)\",\n",
        "                marker=dict(color=consistency_scores, colorscale=\"Viridis\")\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Response times\n",
        "        for agent in agents:\n",
        "            if agent.performance_metrics[\"response_times\"]:\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=list(range(len(agent.performance_metrics[\"response_times\"]))),\n",
        "                        y=agent.performance_metrics[\"response_times\"],\n",
        "                        mode=\"lines+markers\",\n",
        "                        name=f\"{agent.agent_id} Response Time\"\n",
        "                    ),\n",
        "                    row=1, col=2\n",
        "                )\n",
        "\n",
        "        # Coordination effects\n",
        "        if coordination_effects:\n",
        "            gammas = [ce[\"coordination_effect\"] for ce in coordination_effects]\n",
        "            timestamps = [ce[\"timestamp\"] for ce in coordination_effects]\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=timestamps,\n",
        "                    y=gammas,\n",
        "                    mode=\"lines+markers\",\n",
        "                    name=\"Coordination Effect (γ)\",\n",
        "                    line=dict(color=\"red\")\n",
        "                ),\n",
        "                row=2, col=1\n",
        "            )\n",
        "\n",
        "        # Success rates\n",
        "        success_rates = []\n",
        "        for agent in agents:\n",
        "            total = agent.performance_metrics[\"success_count\"] + agent.performance_metrics[\"error_count\"]\n",
        "            if total > 0:\n",
        "                success_rates.append(agent.performance_metrics[\"success_count\"] / total)\n",
        "            else:\n",
        "                success_rates.append(0.0)\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=agent_names,\n",
        "                y=success_rates,\n",
        "                name=\"Success Rate\",\n",
        "                marker=dict(color=success_rates, colorscale=\"RdYlGn\")\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        fig.update_layout(height=800, title_text=\"CERT Multi-Agent Performance Dashboard\")\n",
        "        return fig\n",
        "\n",
        "class PDFProcessor:\n",
        "    \"\"\"Handles PDF upload and text extraction\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def upload_and_extract() -> Dict[str, str]:\n",
        "        \"\"\"Upload PDF and extract text content\"\"\"\n",
        "        print(\"📄 Upload PDF documents for analysis...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        documents = {}\n",
        "        for filename, content in uploaded.items():\n",
        "            if filename.endswith('.pdf'):\n",
        "                try:\n",
        "                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))\n",
        "                    text = \"\"\n",
        "                    for page in pdf_reader.pages:\n",
        "                        text += page.extract_text() + \"\\n\"\n",
        "\n",
        "                    documents[filename] = text\n",
        "                    print(f\"✅ Extracted {len(text)} characters from {filename}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error processing {filename}: {str(e)}\")\n",
        "            else:\n",
        "                print(f\"⚠️ Skipping {filename} (not a PDF)\")\n",
        "\n",
        "        return documents\n",
        "\n",
        "# Available Claude Models\n",
        "CLAUDE_MODELS = {\n",
        "    \"claude-opus-4-20250514\": \"Claude Opus 4 (Latest, Most Powerful)\",\n",
        "    \"claude-sonnet-4-20250514\": \"Claude Sonnet 4 (Latest, Balanced)\",\n",
        "    \"claude-3-5-haiku-20241022\": \"Claude 3.5 Haiku (Fast)\",\n",
        "    \"claude-3-7-sonnet-20250219\": \"Claude 3.7 Sonnet\",\n",
        "    \"claude-3-5-sonnet-20241022\": \"Claude 3.5 Sonnet\",\n",
        "    \"claude-3-5-sonnet-20240620\": \"Claude 3.5 Sonnet (June)\",\n",
        "    \"claude-3-haiku-20240307\": \"Claude 3 Haiku (Legacy)\"\n",
        "}\n",
        "\n",
        "# Configuration Interface\n",
        "def create_agent_config_interface(max_agents: int = 10):\n",
        "    \"\"\"Create interactive interface for configuring agents\"\"\"\n",
        "\n",
        "    def create_agent_widgets(agent_num: int):\n",
        "        agent_id = widgets.Text(\n",
        "            value=f\"agent_{agent_num}\",\n",
        "            description=f\"Agent {agent_num} ID:\",\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        model = widgets.Dropdown(\n",
        "            options=list(CLAUDE_MODELS.keys()),\n",
        "            value=\"claude-sonnet-4-20250514\",\n",
        "            description=f\"Model:\",\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        role = widgets.Text(\n",
        "            value=f\"Analyst {agent_num}\",\n",
        "            description=f\"Role:\",\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        task = widgets.Textarea(\n",
        "            value=f\"Analyze documents and provide insights based on your specialized perspective\",\n",
        "            description=f\"Task:\",\n",
        "            style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"id\": agent_id,\n",
        "            \"model\": model,\n",
        "            \"role\": role,\n",
        "            \"task\": task\n",
        "        }\n",
        "\n",
        "    # Number of agents selector\n",
        "    num_agents = widgets.IntSlider(\n",
        "        value=3,\n",
        "        min=2,\n",
        "        max=max_agents,\n",
        "        description=\"Number of Agents:\",\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    # Global task configuration\n",
        "    global_task = widgets.Textarea(\n",
        "        value=\"Analyze the uploaded PDF document and provide comprehensive insights\",\n",
        "        description=\"Global Task:\",\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    coordination_pattern = widgets.Dropdown(\n",
        "        options=[\"sequential\", \"parallel\"],\n",
        "        value=\"sequential\",\n",
        "        description=\"Coordination Pattern:\",\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"num_agents\": num_agents,\n",
        "        \"global_task\": global_task,\n",
        "        \"coordination_pattern\": coordination_pattern,\n",
        "        \"create_agent_widgets\": create_agent_widgets\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnQZuDMDXdDD"
      },
      "outputs": [],
      "source": [
        "async def run_claude_cert_demo(\n",
        "    api_key: str,\n",
        "    agent_configs: List[Dict[str, str]],\n",
        "    global_task: str,\n",
        "    coordination_pattern: str = \"sequential\",\n",
        "    consistency_trials: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Run the complete Claude-only CERT coordination demonstration\n",
        "\n",
        "    Args:\n",
        "        api_key: Anthropic API key\n",
        "        agent_configs: List of agent configurations with id, model, role, task\n",
        "        global_task: Overall coordination task\n",
        "        coordination_pattern: \"sequential\" or \"parallel\"\n",
        "        consistency_trials: Number of trials for consistency measurement\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"🎯 Claude-Only CERT Multi-Agent Coordination Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Agents: {len(agent_configs)}\")\n",
        "    print(f\"Pattern: {coordination_pattern}\")\n",
        "    print(f\"Task: {global_task}\")\n",
        "    print()\n",
        "\n",
        "    # Create agents\n",
        "    agents = []\n",
        "    for config in agent_configs:\n",
        "        agent = ClaudeAgent(\n",
        "            agent_id=config[\"id\"],\n",
        "            model=config[\"model\"],\n",
        "            role=config[\"role\"],\n",
        "            task_description=config[\"task\"],\n",
        "            api_key=api_key\n",
        "        )\n",
        "        agents.append(agent)\n",
        "        print(f\"✅ Created {config['id']} using {CLAUDE_MODELS[config['model']]}\")\n",
        "\n",
        "    # Upload and process documents\n",
        "    documents = PDFProcessor.upload_and_extract()\n",
        "\n",
        "    if not documents:\n",
        "        print(\"❌ No documents uploaded\")\n",
        "        return None\n",
        "\n",
        "    # Take first document for analysis\n",
        "    doc_name, doc_content = list(documents.items())[0]\n",
        "    print(f\"\\n📄 Analyzing: {doc_name}\")\n",
        "\n",
        "    # Phase 1: Individual agent consistency measurement\n",
        "    print(\"\\n🔍 Phase 1: Individual Agent Analysis\")\n",
        "    for agent in agents:\n",
        "        print(f\"  Testing {agent.agent_id}...\")\n",
        "        consistency = await agent.measure_consistency(global_task, consistency_trials)\n",
        "        print(f\"    Consistency (β): {consistency:.3f}\")\n",
        "\n",
        "    # Phase 2: Coordination measurement\n",
        "    print(f\"\\n🤝 Phase 2: Multi-Agent Coordination ({coordination_pattern})\")\n",
        "\n",
        "    orchestrator = CoordinationOrchestrator(agents)\n",
        "\n",
        "    if coordination_pattern == \"sequential\":\n",
        "        coordination_steps = await orchestrator.run_sequential_coordination(global_task, doc_content)\n",
        "    else:\n",
        "        coordination_steps = await orchestrator.run_parallel_coordination(global_task, doc_content)\n",
        "\n",
        "    # Calculate coordination effect\n",
        "    gamma = orchestrator.measure_coordination_effect(coordination_steps)\n",
        "    print(f\"Coordination Effect (γ): {gamma:.3f}\")\n",
        "\n",
        "    # Phase 3: Generate visualizations\n",
        "    print(\"\\n📊 Phase 3: Generating Interactive Visualizations\")\n",
        "\n",
        "    # Conversation timeline\n",
        "    timeline_fig = CERTVisualizer.create_conversation_timeline(orchestrator.conversation_log)\n",
        "\n",
        "    # Performance dashboard\n",
        "    dashboard_fig = CERTVisualizer.create_performance_dashboard(agents, orchestrator.coordination_effects)\n",
        "\n",
        "    # Display results\n",
        "    display(HTML(\"<h2>🎯 CERT Claude-Only Coordination Results</h2>\"))\n",
        "    display(HTML(f\"<p><b>Document:</b> {doc_name}</p>\"))\n",
        "    display(HTML(f\"<p><b>Agents:</b> {len(agents)} | <b>Pattern:</b> {coordination_pattern}</p>\"))\n",
        "\n",
        "    print(\"\\n📈 Agent Conversation Timeline:\")\n",
        "    display(timeline_fig)\n",
        "\n",
        "    print(\"\\n📊 Performance Dashboard:\")\n",
        "    display(dashboard_fig)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n📋 Summary Statistics:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    for agent in agents:\n",
        "        if agent.performance_metrics[\"consistency_scores\"]:\n",
        "            avg_consistency = np.mean(agent.performance_metrics[\"consistency_scores\"])\n",
        "            avg_response_time = np.mean(agent.performance_metrics[\"response_times\"])\n",
        "            success_rate = agent.performance_metrics[\"success_count\"] / (\n",
        "                agent.performance_metrics[\"success_count\"] + agent.performance_metrics[\"error_count\"]\n",
        "            )\n",
        "\n",
        "            print(f\"{agent.agent_id}:\")\n",
        "            print(f\"  • Model: {CLAUDE_MODELS[agent.model]}\")\n",
        "            print(f\"  • Consistency (β): {avg_consistency:.3f}\")\n",
        "            print(f\"  • Avg Response Time: {avg_response_time:.2f}s\")\n",
        "            print(f\"  • Success Rate: {success_rate:.2%}\")\n",
        "\n",
        "    print(f\"\\nOverall Coordination Effect (γ): {gamma:.3f}\")\n",
        "\n",
        "    if gamma > 1.0:\n",
        "        print(\"✅ Positive coordination detected - agents enhance each other's performance\")\n",
        "    elif gamma < 1.0:\n",
        "        print(\"⚠️ Negative coordination detected - agents may be interfering with each other\")\n",
        "    else:\n",
        "        print(\"➡️ Neutral coordination - no significant coordination effect\")\n",
        "\n",
        "    # Return complete results\n",
        "    return {\n",
        "        \"agents\": agents,\n",
        "        \"orchestrator\": orchestrator,\n",
        "        \"coordination_steps\": coordination_steps,\n",
        "        \"coordination_effect\": gamma,\n",
        "        \"conversation_log\": orchestrator.conversation_log,\n",
        "        \"documents\": documents\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGGWRtm4bbPS"
      },
      "source": [
        "## Run the Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awmM8BKlc4ju"
      },
      "source": [
        "###**Example Configuration**\n",
        "```\n",
        "agent_configs = [\n",
        "    {\n",
        "        \"id\": \"primary_analyst\",\n",
        "        \"model\": \"claude-sonnet-4-20250514\",\n",
        "        \"role\": \"Primary Document Analyst\",\n",
        "        \"task\": \"Extract key themes and main arguments from documents\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"critical_reviewer\",\n",
        "        \"model\": \"claude-3-5-haiku-20241022\",\n",
        "        \"role\": \"Critical Reviewer\",\n",
        "        \"task\": \"Identify gaps, contradictions, and areas needing clarification\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"synthesizer\",\n",
        "        \"model\": \"claude-opus-4-20250514\",\n",
        "        \"role\": \"Synthesis Specialist\",\n",
        "        \"task\": \"Integrate multiple perspectives into coherent conclusions\"\n",
        "    }\n",
        "]\n",
        "\n",
        "global_task = \"Analyze the uploaded document for strategic insights and actionable recommendations\"\n",
        "\n",
        "run_claude_cert_demo(\n",
        "    api_key: str,\n",
        "    agent_configs: agent_configs,\n",
        "    global_task: global_task,\n",
        "    coordination_pattern: str = \"sequential\" #Optional:Parallel - All agents analyze simultaneously, independent perspectives\n",
        "    consistency_trials: int = 3\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NToa9RBseS_k"
      },
      "outputs": [],
      "source": [
        "agent_configs = [\n",
        "    {\n",
        "        \"id\": \"primary_analyst\",\n",
        "        \"model\": \"claude-sonnet-4-20250514\",\n",
        "        \"role\": \"Primary Document Analyst\",\n",
        "        \"task\": \"Extract key themes and main arguments from documents\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"critical_reviewer\",\n",
        "        \"model\": \"claude-3-5-haiku-20241022\",\n",
        "        \"role\": \"Critical Reviewer\",\n",
        "        \"task\": \"Identify gaps, contradictions, and areas needing clarification\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"synthesizer\",\n",
        "        \"model\": \"claude-opus-4-20250514\",\n",
        "        \"role\": \"Synthesis Specialist\",\n",
        "        \"task\": \"Integrate multiple perspectives into coherent conclusions\"\n",
        "    }\n",
        "]\n",
        "\n",
        "global_task = \"Analyze the uploaded document for strategic insights and actionable recommendations\"\n",
        "\n",
        "run_claude_cert_demo(\n",
        "    api_key: ANTHROPIC_API_KEY,\n",
        "    agent_configs: agent_configs,\n",
        "    global_task: global_task,\n",
        "    coordination_pattern: str = \"sequential\" #Optional:Parallel - All agents analyze simultaneously, independent perspectives\n",
        "    consistency_trials: int = 3\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
